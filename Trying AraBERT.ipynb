{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "487a226b",
   "metadata": {},
   "source": [
    "# Using AraBERT for Word and Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58a480ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'arabert'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/aub-mind/arabert.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b1a428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import our needed modules and libraries\n",
    "import farasa\n",
    "import pyarabic\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566decac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-23 02:05:46,026 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name=\"aubmindlab/bert-base-arabertv2\"\n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name) # define the preprocessing model\n",
    "model = AutoModel.from_pretrained(model_name) # define the model \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # define the tokenizer that change the text to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2c11ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " كان الملك رجلا و الملكة مرأة\n",
      "---------------------\n",
      "كان ال+ ملك رجل +ا و ال+ ملك +ة مرأ +ة\n"
     ]
    }
   ],
   "source": [
    "# preprocess sample text using the preprocessing model we defined\n",
    "text =  \" \"\"كان الملك رجلا و الملكة مرأة\"\n",
    "text_preprocessed = arabert_prep.preprocess(text)\n",
    "print(text)\n",
    "print(\"---------------------\")\n",
    "print(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2faae3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting the text to tensors suitable for model input\n",
    "inputs = tokenizer.encode_plus(text_preprocessed, return_tensors='pt')\n",
    "inputs.keys()\n",
    "#inputs is a dictionary containing inputs_ids, attention_masks and token_type_ids as pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "946288bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  33,  369,   20,  880, 1486,    0,  166,   20,  880,   12, 1637,   12,\n",
      "          34])\n",
      "['[CLS]', 'كان', 'ال+', 'ملك', 'رجل', '+ا', 'و', 'ال+', 'ملك', '+ة', 'مرأ', '+ة', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(inputs['input_ids'][0]) # we have only 1 sentence consists of 11 segments and start token [CLS] and end token [SEP]\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad79b4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**inputs) # pass the preprocessed text tensors to the model\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d62760c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = outputs['last_hidden_state'] # contains the embeddings for each individual word\n",
    "embeddings.shape # batch_size (number of sentences) x seq_len (sentence length) x emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c764c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2024, -0.3766, -0.3106,  ..., -0.1226, -0.2897,  0.0998],\n",
       "         [ 0.4053,  0.1943,  0.0517,  ...,  0.3365,  0.1256, -0.5971],\n",
       "         [-0.1299, -0.0100,  0.1046,  ..., -0.0133, -0.7689, -0.1409],\n",
       "         ...,\n",
       "         [-0.1666,  0.0128,  0.2723,  ...,  0.3316, -0.6353, -0.8559],\n",
       "         [-0.1299, -0.0102,  0.1046,  ..., -0.0136, -0.7685, -0.1406],\n",
       "         [ 0.2700, -0.0583, -0.4694,  ...,  0.1925, -0.0790, -0.0127]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c369aa1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_text_only = outputs['last_hidden_state'][0][1:-1] # without [CLS] and [SEP]\n",
    "embeddings_text_only.shape # (seq_len - 2) x emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "759a3323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_vector = outputs['pooler_output'] # has the embeddings of the whole sentences\n",
    "pooled_vector.shape # batch_size x emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54417530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
